\section{Space object recognition} \label{sec:spacerecognition}
Object recognition refers to a series of computer vision tasks, whose goal is to identify objects on images. It consists of two separate tasks: object localization and classification. 
The first step of object recognition is to find all objects present in the image, which is the goal of the object localization task. The algorithm outputs the location of the object as well as the bounding box encapsulating the object. Each object is then assigned a label that defines the class that the object belongs to. This is called classification. 
%Next, features from each object need to be extracted. This can be done by means of traditional or deep learning methods. Based on these features, each object is assigned a label that defines the class that the object belongs to. This is called classification. 

%Object recognition is used in many areas of life. 
%% nieco kde sa to pouziva a potom ze sa to pouziva aj v astronomical field

In this chapter, we will talk about various approaches to object recognition through astronomical images or features extracted from them. %We will focus on two main things: the type of data and the proposed method. 

\subsection{Traditional methods}
For many years traditional analytical methods were used for object recognition. These methods relied on astronomersâ€˜ knowledge of the given task and were designed for specific types of space objects. 

\subsubsection{On-orbit recognition of resident space objects by using star trackers}
The main goal of the article \cite{SPILLER2020478} was to evaluate the possibility of using star trackers to track resident space objects (RSO) in space. For this purpose, they developed an algorithm that could operate on board with limited computational performance and memory.
With this in mind, the algorithm could not be based on machine learning as this would be computationally demanding. 

In order to test the proposed algorithm, synthetic images needed to be generated using their own star-tracker hardware simulator. 
The simulator consists of two modules:
\begin{itemize}
    \item Sky and spacecrafts input generator
    \item High-fidelity image generator

\end{itemize}

The goal of the first block is to simulate the sky, stars, and RSOs. The sky simulation is performed using a star catalog while considering the position and orientation of the sensor as well as optical and geometric characteristics. A similar procedure is followed when simulating real RSOs, but using the catalog with orbiting RSOs instead of the star catalog. Another way of simulating RSOs is to use the grid method, which simulates fictitious RSOs. 
The second block is used to make the image realistic. The block receives the list of stars and RSOs with their position and magnitude. Using this data as well as some additional information regarding noises, optical, geometrical, and electronic characteristics of the sensor, the block produces high fidelity synthetic images. 

When it comes to the developed algorithm, the first step is to identify objects of interest from the background. After the identification of objects is complete, a list of objects and their positions is produced for every image. 
The algorithm is then given pairs of images and compares each object on the first image to each object in the second image. Based on some predetermined conditions regarding their position, distance, velocity, and density, objects are matched. This means that the object in the first image is the same object in the second image. The algorithm performs this comparison for a series of pairs of images and as a result, the object is being tracked.

This method was mainly developed to track moving objects in space-based observations. In this situation, RSOs usually appear as streaks and the algorithm was optimized for this scenario. However, this poses a disadvantage since point-like objects are not recognized well and diffuse sources are considered background noise. 

\subsection{Machine learning methods}
With an increase in the amount of data, the traditional methods weren't fast and robust enough. Machine learning methods were able to tackle this issue, by learning on their own. Extracting knowledge from a large amount of data allows them to find patterns in data that may not be visible to humans. This way they can outperform even the best traditional methods \cite{Goodfellow-et-al-2016}.

 
\subsubsection{Galaxy morphology classification using automated machine learning} 
In \cite{REZA2021100492}, multiple machine learning algorithms were tested with an aim to classify galaxies into four types. The study was conducted in order to assess the possibility of using ML for future surveys. 

Dataset used to train models contained more than 304 000 samples of galaxies of different types (spirals, ellipticals, mergers, and stars). Feature vectors were obtained from SDSS (Sloan Digital Sky Survey) and their extraction wasn't explained since that is not the main subject of the article. %Feature vector included features such as fiber, model and Petrosian colors, axis ratio, degree of ellipticity, and others. Explanation of each feature is provided in the mentioned article \cite{REZA2021100492} and will not be done here since they are not relevant to this thesis. 

Five different ML methods were chosen for the article - Decision trees, Random Forest, ExtraTrees, K-nearest neighbors, and Artificial Neural Network. Before the data was fed to any model, PCA was performed on feature vectors to reduce their dimensionality. As a result, 25 most significant features were used from the original feature vector of size 62. As usual, models were trained on the training set and hyperparameters were tuned on the separate validation set. 
Evaluating the models on the testing set and comparing the accuracy showed that ANN had the best results. Even though the overall accuracy of the ANN was the best, the accuracy of minority class samples showed poor results. In this case, ensemble methods like a combination of ExtraTrees with Random Forest performed much better. In conclusion, we can say that using a balanced dataset with a large number of samples could prove to be useful when working with ANN. 




\subsubsection{Applications of neural networks to object detection and star/galaxy classification}
%https://academic.oup.com/mnras/article/319/3/700/1073630?login=false
In the article, \cite{Andreon2000}, the developed package called Neural extractor (NExt) was presented. The package can perform object detection and star/galaxy classification. The authors used three different NNs for each specific task: data reduction, detection, and classification. 

For the training, validation, and testing, the subset of the IP92 catalog \cite{1992ApJS} was used. To train the detection part of the algorithm, the best solution was to use around 10 subimages 50x50 pixels wide. To test the performance of the detection and classification networks, 4819 and 460 objects from the catalog were chosen, respectively.

The first step of the package included the detection of desired objects. This task was performed as a classification of pixels into background and object class. First, the non-linear PCA NN was applied to the image, which reduced the redundant information in nearby pixels. Transformed values of the pixels were then fed to unsupervised NN. Multiple different models were tested and the best performing one was a multi-layer neural gas network with a running window of size 3 or 5. 
This network aimed to classify pixels into background or object class. This was done in an unsupervised matter and the network has split pixels into multiple classes. One of the classes could be described as a background, whereas the others described different kinds of astronomical objects and were later merged to form an object class. After the segmentation, the overlapping objects were detected and deblended.
The second step included feature extraction and the star/galaxy classification. Considering feature extraction, multiple features were measured, and through the sequential backward elimination strategy, the best-performing ones were selected. These features were then used to train MLP to perform the final task of star/galaxy classification. 

To test the performance of the proposed method, the authors compared the results to the best performing package at the time (SExtractor \cite{sextractor}). Considering the detection phase of the algorithm, the proposed method was as effective as the SExtractor in the detection of true objects. For the classification phase, the NExt performed better than the SExtractor, with only 28 errors compared to 41 for the SExtractor from the total of 480 objects. 

It wasn't explicitly mentioned in the article but according to pictures, this method considers the only point-like appearance of the stars. This poses a huge disadvantage in our case since the space debris usually appears as streaks during the sidereal tracking.  



\subsubsection{Deblending and classifying astronomical sources with Mask R-CNN deep learning}
In \cite{Burke2019}, a network based on Mask Region-based CNN (Mask R-CNN) framework is used to detect, classify and deblend star and galaxy sources. 

The network is trained, validated, and tested using simulated images generated by The Photon Simulator. The simulator generates 512x512 three-band images, where each image contains around 150 objects. For the training set, 1000 images were simulated, which is approximately 150 000 astronomical sources. The validation and testing set contains 250 and 50 simulated images, respectively. 

The Mask R-CNN \cite{maskrcnn2017} comes from a family of region-based CNN frameworks, that specialize in the task of object recognition. The architecture of the R-CNN framework could be summarized as multiple subnetworks, where each has its own specific task. The backbone of the framework is a deep CNN, whose goal is to extract features from the image. The region proposal network finds regions in the image and the type of object in the area. At the end of the framework, there are 2 subnetworks, one predicts the type of object in the proposed region, and the other outputs the bounding box of the object. Mask R-CNN is an extension of R-CNN frameworks, with additional subnetwork for instance segmentation.
In the article, the backbone used for Mask R-CNN is a Resnet with 101 layers. Transfer learning was used to improve the speed of the training process, with initial weights trained on the MS COCO dataset.  

The network was tested on simulated images and achieved a precision of 92 \% on star sources and 98 \% on galaxies. Results also revealed that the network performs significantly better with small sources, which could be due to a few large examples in the training set. 

